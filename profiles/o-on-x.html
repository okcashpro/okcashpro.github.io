
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>o-on-x - GitHub Contributions</title>
    <script crossorigin src="https://unpkg.com/react@18/umd/react.production.min.js"></script>
    <script crossorigin src="https://unpkg.com/react-dom@18/umd/react-dom.production.min.js"></script>
    <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
</head>
<body class="bg-gray-50 dark:bg-gray-900">
    <div id="root"><div class="max-w-7xl mx-auto p-4 space-y-6"><div class="bg-white dark:bg-gray-800 rounded-lg p-6 shadow"><div class="flex items-center justify-between"><div class="flex items-center gap-4"><img src="https://avatars.githubusercontent.com/u/186759703?v=4" alt="o-on-x&#x27;s avatar" class="w-16 h-16 rounded-full"/><div><h1 class="text-2xl font-bold">o-on-x</h1><div class="text-gray-600 dark:text-gray-400"><span class="font-semibold">Score: </span>163</div></div></div><div class="text-3xl font-bold text-blue-600 dark:text-blue-400">163</div></div></div><div class="bg-white dark:bg-gray-800 rounded-lg p-6 shadow"><p class="text-gray-700 dark:text-gray-300 text-sm leading-relaxed">o-on-x is a developer actively contributing to a GitHub repository with a focus on AI and tweet-related functionalities. Their recent work includes adding style guidelines to the context, removing specific embeddings, setting post times, and addressing issues related to bot interactions and response handling. They seem to be engaged in backend development, bug fixes, and feature enhancements, with a particular emphasis on improving tweet generation and AI functionality.</p></div><div class="grid grid-cols-1 md:grid-cols-4 gap-4"><div class="bg-white dark:bg-gray-800 rounded-lg p-6 shadow"><h3 class="font-semibold">Commits</h3><p class="text-2xl font-bold">41</p></div><div class="bg-white dark:bg-gray-800 rounded-lg p-6 shadow"><h3 class="font-semibold">Pull Requests</h3><p class="text-2xl font-bold">20</p></div><div class="bg-white dark:bg-gray-800 rounded-lg p-6 shadow"><h3 class="font-semibold">Issues</h3><p class="text-2xl font-bold">8</p></div><div class="bg-white dark:bg-gray-800 rounded-lg p-6 shadow"><h3 class="font-semibold">Comments</h3><p class="text-2xl font-bold">14</p></div></div><div class="space-y-4"><div class="border rounded-lg p-4"><div class="flex items-center justify-between cursor-pointer"><h3 class="font-semibold">Commits</h3><span>▶</span></div></div><div class="border rounded-lg p-4"><div class="flex items-center justify-between cursor-pointer"><h3 class="font-semibold">Pull Requests</h3><span>▶</span></div></div><div class="border rounded-lg p-4"><div class="flex items-center justify-between cursor-pointer"><h3 class="font-semibold">Issues</h3><span>▶</span></div></div><div class="border rounded-lg p-4"><div class="flex items-center justify-between cursor-pointer"><h3 class="font-semibold">Comments</h3><span>▶</span></div></div></div></div></div>
    <script>
        window.__DATA__ = {"contributor":"o-on-x","avatar_url":"https://avatars.githubusercontent.com/u/186759703?v=4","activity":{"code":{"commits":[{"sha":"1ae22ee8ebb8909bb1d204ae49ece150f7357bdf","date":"2024-11-20T07:46:57Z","message":"Feature Modification: Add all the stlye guidelines to the context","url":"https://github.com/ai16z/eliza/commit/1ae22ee8ebb8909bb1d204ae49ece150f7357bdf"},{"sha":"df57d56429397e2c7b2bda33f4a4b87a0a85c55c","date":"2024-11-19T04:57:15Z","message":"removed ollama embeddings. fastembeddings or openai only","url":"https://github.com/ai16z/eliza/commit/df57d56429397e2c7b2bda33f4a4b87a0a85c55c"},{"sha":"63918b98bce5d7768a864f08d15418b9c303da67","date":"2024-11-17T01:49:38Z","message":"Merge pull request #369 from o-on-x/main\n\ndefault set to new standard  post time 90-180 type: post time","url":"https://github.com/ai16z/eliza/commit/63918b98bce5d7768a864f08d15418b9c303da67"},{"sha":"145dba5e6435026a237fbfbe50b9595f7d2a9175","date":"2024-11-17T01:46:50Z","message":"default set to new standard  post time 90-180","url":"https://github.com/ai16z/eliza/commit/145dba5e6435026a237fbfbe50b9595f7d2a9175"},{"sha":"7bc8edcddb989aeac34a03eb8f3d80790dd54304","date":"2024-11-17T01:44:09Z","message":"Merge pull request #368 from o-on-x/main\n\npost time set in env","url":"https://github.com/ai16z/eliza/commit/7bc8edcddb989aeac34a03eb8f3d80790dd54304"},{"sha":"1a905095abb821416a1c66ec7dc0f5ab3b7ab5a8","date":"2024-11-17T01:42:33Z","message":"post time set in env","url":"https://github.com/ai16z/eliza/commit/1a905095abb821416a1c66ec7dc0f5ab3b7ab5a8"},{"sha":"46a913e4d5a15ce929974732b2303103467f7f50","date":"2024-11-15T01:29:33Z","message":"Merge branch 'ai16z:main' into o-on-x-wip-tweet-split","url":"https://github.com/ai16z/eliza/commit/46a913e4d5a15ce929974732b2303103467f7f50"},{"sha":"bf3ed27015bcabe89492540de23f9c9cab61f833","date":"2024-11-15T01:26:00Z","message":"utils.ts example tweet splitting\n\nif no other splitting is done in the other modules would work\r\ninteractions.ts & post.ts should then just generate any size no character limit tweet (within reason)\r\nwip","url":"https://github.com/ai16z/eliza/commit/bf3ed27015bcabe89492540de23f9c9cab61f833"},{"sha":"8805eb4e7eef62e441cda19f05da99d517f4fac0","date":"2024-11-14T09:50:47Z","message":"Merge pull request #313 from o-on-x/main\n\nadded working pumpfun.ts","url":"https://github.com/ai16z/eliza/commit/8805eb4e7eef62e441cda19f05da99d517f4fac0"},{"sha":"79637da76cdc52631b192c1ea323fb4eda23f0ff","date":"2024-11-14T09:42:07Z","message":"added working pumpfun.ts","url":"https://github.com/ai16z/eliza/commit/79637da76cdc52631b192c1ea323fb4eda23f0ff"},{"sha":"1b0d5fe451a803fea480afa6be464805ff4fa2bc","date":"2024-11-14T01:15:39Z","message":"image gen saves file & returns format for clients; discord image poasting just works","url":"https://github.com/ai16z/eliza/commit/1b0d5fe451a803fea480afa6be464805ff4fa2bc"},{"sha":"7875865f1c8629a41cd0abcba7f5c342d40db6d0","date":"2024-11-14T00:40:32Z","message":"Merge pull request #304 from o-on-x/main\n\ntelegram: start agent after client initialization","url":"https://github.com/ai16z/eliza/commit/7875865f1c8629a41cd0abcba7f5c342d40db6d0"},{"sha":"83152cc2ed04a3ff4252842f10500856ad2837b2","date":"2024-11-14T00:37:43Z","message":"tg.start() is added; agents starts","url":"https://github.com/ai16z/eliza/commit/83152cc2ed04a3ff4252842f10500856ad2837b2"},{"sha":"b84caacaf00ad5a9ec08b44d1c9b39e0c655617a","date":"2024-11-13T22:42:53Z","message":"Merge pull request #297 from o-on-x/main\n\nAdded Transfer / Send Token Action","url":"https://github.com/ai16z/eliza/commit/b84caacaf00ad5a9ec08b44d1c9b39e0c655617a"},{"sha":"d5d6798a5f0f386dfb24c45f0b4b6ddad93a6630","date":"2024-11-13T22:40:35Z","message":"Transfer Token / Send Token Action Added","url":"https://github.com/ai16z/eliza/commit/d5d6798a5f0f386dfb24c45f0b4b6ddad93a6630"},{"sha":"9caf91f03a0a2e1ce388d4bd84504b66192aefbb","date":"2024-11-13T22:39:19Z","message":"Update index.ts with transferToken action to use","url":"https://github.com/ai16z/eliza/commit/9caf91f03a0a2e1ce388d4bd84504b66192aefbb"},{"sha":"6f53ba0e817871124951c1ba4f29674a2aaca335","date":"2024-11-11T05:17:49Z","message":"Merge pull request #256 from o-on-x/main\n\nbigint support in logger","url":"https://github.com/ai16z/eliza/commit/6f53ba0e817871124951c1ba4f29674a2aaca335"},{"sha":"a27ba5f862de51e6614b753f067f49e50758471a","date":"2024-11-11T05:16:55Z","message":"comment out debugging logging on embeddings","url":"https://github.com/ai16z/eliza/commit/a27ba5f862de51e6614b753f067f49e50758471a"},{"sha":"f3a0269c05f87db942310d43a1bf42390cbe5bba","date":"2024-11-11T05:08:34Z","message":"bigint support in logger","url":"https://github.com/ai16z/eliza/commit/f3a0269c05f87db942310d43a1bf42390cbe5bba"},{"sha":"86d8be94e066cec9d34c831b248287857fec2d37","date":"2024-11-11T02:23:57Z","message":"Merge pull request #255 from o-on-x/main\n\nembedding set to use openai endpoint when using openai embeddings","url":"https://github.com/ai16z/eliza/commit/86d8be94e066cec9d34c831b248287857fec2d37"},{"sha":"ad0a45e7887766bd47f3012ce17b039ad47719c2","date":"2024-11-11T02:20:22Z","message":"embedding set to use openai endpoint when using openai embeddings","url":"https://github.com/ai16z/eliza/commit/ad0a45e7887766bd47f3012ce17b039ad47719c2"},{"sha":"420399e9ea83dcda1e863e70f65e4cb313409d87","date":"2024-11-11T01:36:01Z","message":"Merge pull request #254 from o-on-x/main\n\nrefactor embeddings","url":"https://github.com/ai16z/eliza/commit/420399e9ea83dcda1e863e70f65e4cb313409d87"},{"sha":"7aad2f78040a33fc83f593019bd5e00aba8407bf","date":"2024-11-11T01:33:31Z","message":"Merge branch 'ai16z:main' into main","url":"https://github.com/ai16z/eliza/commit/7aad2f78040a33fc83f593019bd5e00aba8407bf"},{"sha":"21a1fb4de2602007e35fafe84d6b87576ef4f0e3","date":"2024-11-11T01:32:16Z","message":"refactor embeddings to decouple getRemote getLocal for calls regardless of runtime","url":"https://github.com/ai16z/eliza/commit/21a1fb4de2602007e35fafe84d6b87576ef4f0e3"},{"sha":"b512d8febad8c40d506582afc9c900ba591563c3","date":"2024-11-11T00:28:00Z","message":"seed.sql added id field for participants","url":"https://github.com/ai16z/eliza/commit/b512d8febad8c40d506582afc9c900ba591563c3"},{"sha":"acb4e865b66bfb6bf9ba0fa451e29eeceedaf345","date":"2024-11-10T21:52:55Z","message":"Merge pull request #252 from o-on-x/main\n\nuse openai embeddings setting","url":"https://github.com/ai16z/eliza/commit/acb4e865b66bfb6bf9ba0fa451e29eeceedaf345"},{"sha":"432362bf432e0f3ecf34ed1187b5f3f06b74ced4","date":"2024-11-10T21:32:22Z","message":"use openai embeddings setting","url":"https://github.com/ai16z/eliza/commit/432362bf432e0f3ecf34ed1187b5f3f06b74ced4"},{"sha":"ce4d327034fdac702eb239b18d0fd4be9683b015","date":"2024-11-09T18:33:43Z","message":"Merge pull request #245 from o-on-x/main\n\nAdded OpenRouter model provider & BASE_MINT Var","url":"https://github.com/ai16z/eliza/commit/ce4d327034fdac702eb239b18d0fd4be9683b015"},{"sha":"2fb624c5a344557f8331f5b9b1131a0ba6def4b5","date":"2024-11-09T18:31:39Z","message":"Added the BASE_MINT var needed on TrustScoreProvider","url":"https://github.com/ai16z/eliza/commit/2fb624c5a344557f8331f5b9b1131a0ba6def4b5"},{"sha":"14aae057afe8c80c190e0c1bcaa2263a8a9e3a28","date":"2024-11-09T15:21:49Z","message":"Merge branch 'ai16z:main' into main","url":"https://github.com/ai16z/eliza/commit/14aae057afe8c80c190e0c1bcaa2263a8a9e3a28"},{"sha":"bc817e5abc8b64f61442f3a8adc2a32fefa65973","date":"2024-11-09T10:44:57Z","message":"Merge branch 'ai16z:main' into main","url":"https://github.com/ai16z/eliza/commit/bc817e5abc8b64f61442f3a8adc2a32fefa65973"},{"sha":"1f80cbb7a91a9986cd490781be1328bf186cdab1","date":"2024-11-09T10:34:01Z","message":"corrected link for available openrouter models","url":"https://github.com/ai16z/eliza/commit/1f80cbb7a91a9986cd490781be1328bf186cdab1"},{"sha":"7b3fd42411db8f93d61e1629602ec7e8cf584a66","date":"2024-11-09T10:31:04Z","message":"Merge branch 'ai16z:main' into main","url":"https://github.com/ai16z/eliza/commit/7b3fd42411db8f93d61e1629602ec7e8cf584a66"},{"sha":"1fc9c3e85c2df859601c114198839a401671f3ed","date":"2024-11-09T10:28:28Z","message":"OpenRouter corrected model defaults","url":"https://github.com/ai16z/eliza/commit/1fc9c3e85c2df859601c114198839a401671f3ed"},{"sha":"7e08bb4ffbefa07ed81929d7d590f8f992fac802","date":"2024-11-09T10:13:43Z","message":"Added OpenRouter model provider","url":"https://github.com/ai16z/eliza/commit/7e08bb4ffbefa07ed81929d7d590f8f992fac802"},{"sha":"d4268b389cb31a990048cdc8966e7bbe00b68d03","date":"2024-11-07T12:56:29Z","message":"changed plugin-image-gen tsconfig.json & added error handling ollama initialization","url":"https://github.com/ai16z/eliza/commit/d4268b389cb31a990048cdc8966e7bbe00b68d03"},{"sha":"5b79da685d8b6f32788a5b00475ce6e7ce62b6a9","date":"2024-11-07T10:41:38Z","message":"Merge branch 'ai16z:main' into main","url":"https://github.com/ai16z/eliza/commit/5b79da685d8b6f32788a5b00475ce6e7ce62b6a9"},{"sha":"04fabb0cf5ac8a020394bd0a3d7d20d031ddce5a","date":"2024-11-07T10:17:55Z","message":"just fetching upstream & merge","url":"https://github.com/ai16z/eliza/commit/04fabb0cf5ac8a020394bd0a3d7d20d031ddce5a"},{"sha":"1d551e211cf47de4e75fd018d860969bc607f571","date":"2024-11-07T08:40:53Z","message":"tsconfig ref @eliza/core, target src, include types, & esm module resolution. & models.gguf stored in models folder","url":"https://github.com/ai16z/eliza/commit/1d551e211cf47de4e75fd018d860969bc607f571"},{"sha":"939f73936e4fcf0d11a37d2a9fe7da0970761f1c","date":"2024-11-07T03:46:09Z","message":"Model Provider for OLLAMA","url":"https://github.com/ai16z/eliza/commit/939f73936e4fcf0d11a37d2a9fe7da0970761f1c"},{"sha":"90e29e3d73521d607511c9c1664d5e835d14aba3","date":"2024-11-07T01:01:42Z","message":"Added Switch for llama-cpp & ollama with LOCALLLAMA Provider","url":"https://github.com/ai16z/eliza/commit/90e29e3d73521d607511c9c1664d5e835d14aba3"}],"pull_requests":[{"number":441,"title":"feat:  add all the style guidelines to the context","state":"closed","created_at":"2024-11-20T07:54:13Z","url":"https://github.com/ai16z/eliza/pull/441","labels":[],"comments":0},{"number":413,"title":"fix: removed ollama embeddings. fastembeddings or openai only","state":"closed","created_at":"2024-11-19T05:06:10Z","url":"https://github.com/ai16z/eliza/pull/413","labels":[],"comments":3},{"number":369,"title":"default set to new standard  post time 90-180 type: post time","state":"closed","created_at":"2024-11-17T01:47:24Z","url":"https://github.com/ai16z/eliza/pull/369","labels":[],"comments":0},{"number":368,"title":"post time set in env","state":"closed","created_at":"2024-11-17T01:43:57Z","url":"https://github.com/ai16z/eliza/pull/368","labels":[],"comments":0},{"number":351,"title":"Togetherai returns url. Convert to base64 like openai. Both get saved to file in plugin","state":"closed","created_at":"2024-11-16T06:28:12Z","url":"https://github.com/ai16z/eliza/pull/351","labels":[],"comments":0},{"number":350,"title":"Added Min Max Settings for Poast Time","state":"closed","created_at":"2024-11-16T06:16:04Z","url":"https://github.com/ai16z/eliza/pull/350","labels":[],"comments":0},{"number":324,"title":"tweet split ","state":"closed","created_at":"2024-11-15T01:32:27Z","url":"https://github.com/ai16z/eliza/pull/324","labels":[],"comments":0},{"number":323,"title":"utils.ts example tweet splitting","state":"closed","created_at":"2024-11-15T01:27:23Z","url":"https://github.com/ai16z/eliza/pull/323","labels":[],"comments":1},{"number":313,"title":"added working pumpfun.ts","state":"closed","created_at":"2024-11-14T09:50:31Z","url":"https://github.com/ai16z/eliza/pull/313","labels":[],"comments":0},{"number":306,"title":"image gen saves file & returns format for clients; discord image poasting just works","state":"closed","created_at":"2024-11-14T01:20:21Z","url":"https://github.com/ai16z/eliza/pull/306","labels":[],"comments":0},{"number":304,"title":"telegram: start agent after client initialization","state":"closed","created_at":"2024-11-14T00:40:22Z","url":"https://github.com/ai16z/eliza/pull/304","labels":[],"comments":0},{"number":297,"title":"Added Transfer / Send Token Action","state":"closed","created_at":"2024-11-13T22:42:39Z","url":"https://github.com/ai16z/eliza/pull/297","labels":[],"comments":0},{"number":256,"title":"bigint support in logger","state":"closed","created_at":"2024-11-11T05:11:11Z","url":"https://github.com/ai16z/eliza/pull/256","labels":[],"comments":0},{"number":255,"title":"embedding set to use openai endpoint when using openai embeddings","state":"closed","created_at":"2024-11-11T02:23:50Z","url":"https://github.com/ai16z/eliza/pull/255","labels":[],"comments":0},{"number":254,"title":"refactor embeddings ","state":"closed","created_at":"2024-11-11T01:35:53Z","url":"https://github.com/ai16z/eliza/pull/254","labels":[],"comments":0},{"number":252,"title":"use openai embeddings setting","state":"closed","created_at":"2024-11-10T21:34:08Z","url":"https://github.com/ai16z/eliza/pull/252","labels":[],"comments":0},{"number":245,"title":"Added OpenRouter model provider","state":"closed","created_at":"2024-11-09T10:14:44Z","url":"https://github.com/ai16z/eliza/pull/245","labels":[],"comments":0},{"number":228,"title":"plugin-image-generation tsconfig.json fix & ollama error handling","state":"closed","created_at":"2024-11-07T13:01:14Z","url":"https://github.com/ai16z/eliza/pull/228","labels":[],"comments":0},{"number":224,"title":"models.gguf stored in models file, & tsconfig changes for ref @eliza/core & other things","state":"closed","created_at":"2024-11-07T08:44:09Z","url":"https://github.com/ai16z/eliza/pull/224","labels":[],"comments":0},{"number":221,"title":"Add OLLAMA as Model Provider ","state":"closed","created_at":"2024-11-07T05:52:51Z","url":"https://github.com/ai16z/eliza/pull/221","labels":[],"comments":0}],"total_commits":41,"total_prs":20},"issues":{"opened":[{"number":438,"title":"Feature: when getting style guidelines should always add all of them to context","state":"open","created_at":"2024-11-20T07:16:21Z","url":"https://github.com/ai16z/eliza/issues/438","labels":["enhancement"],"comments":1},{"number":399,"title":"way for bots to have cool down periods (dynamic tempature adjusts) & only direct reply setting","state":"open","created_at":"2024-11-18T12:34:37Z","url":"https://github.com/ai16z/eliza/issues/399","labels":["enhancement"],"comments":0},{"number":319,"title":"no action response found in the response content for twitter or tg clients","state":"open","created_at":"2024-11-14T17:20:45Z","url":"https://github.com/ai16z/eliza/issues/319","labels":["bug"],"comments":0},{"number":238,"title":"Issue with \"cannot read properties of undefined\"","state":"closed","created_at":"2024-11-09T00:58:05Z","url":"https://github.com/ai16z/eliza/issues/238","labels":["bug"],"comments":1},{"number":230,"title":"twitter folder paths for twitter cookies & cache/last tweet point to different places","state":"open","created_at":"2024-11-07T21:42:56Z","url":"https://github.com/ai16z/eliza/issues/230","labels":["bug"],"comments":1},{"number":229,"title":".env is not being loaded or picked up in settings","state":"closed","created_at":"2024-11-07T14:31:33Z","url":"https://github.com/ai16z/eliza/issues/229","labels":["bug"],"comments":2},{"number":227,"title":"agent & plugin-image-generation failed to build","state":"closed","created_at":"2024-11-07T11:58:54Z","url":"https://github.com/ai16z/eliza/issues/227","labels":["bug"],"comments":3},{"number":156,"title":"Set Port Number in Env to Run Multiple Instances","state":"closed","created_at":"2024-11-01T14:40:27Z","url":"https://github.com/ai16z/eliza/issues/156","labels":["enhancement"],"comments":0}],"total_opened":8},"engagement":{"comments":[{"id":2451379559,"body":"Created fork that uses ollama for llama.ts instead of node-llama-cpp. lowers technical debt of having to build llama-cpp & download model. PR might not want to if instead an olllama.ts should be added & not remove the llama-cpp local options.  \r\nhttps://github.com/o-on-x/eliza","created_at":"2024-11-01T06:34:03Z","url":"https://github.com/ai16z/eliza/issues/69#issuecomment-2451379559","type":"issue","issue_number":"69"},{"id":2453268659,"body":"character issues:\r\nneed to have how the image gen is influenced by character file & user requests \r\n\r\nfor now character file already influences if you directly mention about what images it posts & how often in the style part of character file \r\n\r\nupload issues:\r\nthe main issues with functionality are imageGen returns a base64 url string. this needs to be loaded to buffer/file before upload\r\nsecond issue is twitter client needs a upload message addition on the interface which twitter-client-api might support\r\n\r\ncurrent image post working changes I'm using for discord (telegram is similar need to find where i put that code) :     \r\nin src/clients/discord/messages.ts:\r\n\r\n`\r\n\r\n     const callback: HandlerCallback = async (content: Content, files: any[]) => {\r\n\r\n        \r\n        // Process any data URL attachments\r\n        const processedFiles = [...(files || [])];\r\n        \r\n        if (content.attachments?.length) {\r\n          for (const attachment of content.attachments) {\r\n            if (attachment.url?.startsWith('data:')) {\r\n              try {\r\n                const {buffer, type} = await this.attachmentManager.processDataUrlToBuffer(attachment.url);\r\n                const extension = type.split('/')[1] || 'png';\r\n                const fileName = `${attachment.id || Date.now()}.${extension}`;\r\n                \r\n                processedFiles.push({\r\n                  attachment: buffer,\r\n                  name: fileName\r\n                });\r\n                content.text = \"...\"\r\n                // Update the attachment URL to reference the filename\r\n                attachment.url = `attachment://${fileName}`;\r\n              } catch (error) {\r\n                console.error('Error processing data URL:', error);\r\n              }\r\n            }\r\n          }\r\n        }\r\n      \r\n        if (message.id && !content.inReplyTo) {\r\n          content.inReplyTo = stringToUuid(message.id);\r\n        }\r\n      \r\n        if (message.channel.type === ChannelType.GuildVoice) {\r\n          console.log(\"generating voice\");\r\n          const audioStream = await SpeechService.generate(\r\n            this.runtime,\r\n            content.text,\r\n          );\r\n          await this.voiceManager.playAudioStream(userId, audioStream);\r\n          const memory: Memory = {\r\n            id: stringToUuid(message.id),\r\n            userId: this.runtime.agentId,\r\n            content,\r\n            roomId,\r\n            embedding: embeddingZeroVector,\r\n          };\r\n          return [memory];\r\n        } else {\r\n          // For text channels, send the message with the processed files\r\n          const messages = await sendMessageInChunks(\r\n            message.channel as TextChannel,\r\n            content.text,\r\n            message.id,\r\n            processedFiles,\r\n          );\r\n          let notFirstMessage = false;\r\n          const memories: Memory[] = [];\r\n          for (const m of messages) {\r\n            let action = content.action;\r\n            // If there's only one message or it's the last message, keep the original action\r\n            // For multiple messages, set all but the last to 'CONTINUE'\r\n            if (messages.length > 1 && m !== messages[messages.length - 1]) {\r\n              action = \"CONTINUE\";\r\n            }\r\n\r\n            notFirstMessage = true;\r\n            const memory: Memory = {\r\n              id: stringToUuid(m.id),\r\n              userId: this.runtime.agentId,\r\n              content: {\r\n                ...content,\r\n                action,\r\n                inReplyTo: messageId,\r\n                url: m.url,\r\n              },\r\n              roomId,\r\n              embedding: embeddingZeroVector,\r\n              createdAt: m.createdTimestamp,\r\n            };\r\n            memories.push(memory);\r\n          }\r\n          for (const m of memories) {\r\n            await this.runtime.messageManager.createMemory(m);\r\n          }\r\n          return memories;\r\n        }\r\n      };\r\n\r\n\r\n`\r\n\r\nin src/clients/discord/attachments.ts/class AttachmentManager:\r\n\r\n`\r\n  async processDataUrlToBuffer(dataUrl: string): Promise<{buffer: Buffer, type: string}> {\r\n    const matches = dataUrl.match(/^data:([A-Za-z-+\\/]+);base64,(.+)$/);\r\n    \r\n    if (!matches || matches.length !== 3) {\r\n      throw new Error('Invalid data URL');\r\n    }\r\n  \r\n    const type = matches[1];\r\n    const base64Data = matches[2];\r\n    const buffer = Buffer.from(base64Data, 'base64');\r\n    \r\n    return {buffer, type};\r\n  }\r\n`","created_at":"2024-11-03T02:37:47Z","url":"https://github.com/ai16z/eliza/issues/158#issuecomment-2453268659","type":"issue","issue_number":"158"},{"id":2453282609,"body":"the line for catching the image is actually\r\n`if (attachment.url?.startsWith('data:image')) {`","created_at":"2024-11-03T03:48:39Z","url":"https://github.com/ai16z/eliza/issues/158#issuecomment-2453282609","type":"issue","issue_number":"158"},{"id":2453296314,"body":"previous version i had changed the llama.ts to use ollama instead. updated for the latest version\r\ni will look into other ways to do this. but for now this sidesteps using node-llama-cpp & gguf model downloads\r\njust set XAI_MODEL=\"name-of-model\"\r\n\r\n`\r\nimport { OpenAI } from 'openai';\r\nimport * as dotenv from 'dotenv';\r\nimport { debuglog } from 'util';\r\n\r\n// Create debug logger\r\nconst debug = debuglog('LLAMA');\r\n\r\nprocess.on('uncaughtException', (err) => {\r\n  debug('Uncaught Exception:', err);\r\n  process.exit(1);\r\n});\r\n\r\nprocess.on('unhandledRejection', (reason, promise) => {\r\n  debug('Unhandled Rejection at:', promise, 'reason:', reason);\r\n});\r\n\r\ninterface QueuedMessage {\r\n  context: string;\r\n  temperature: number;\r\n  stop: string[];\r\n  max_tokens: number;\r\n  frequency_penalty: number;\r\n  presence_penalty: number;\r\n  useGrammar: boolean;\r\n  resolve: (value: any | string | PromiseLike<any | string>) => void;\r\n  reject: (reason?: any) => void;\r\n}\r\n\r\nclass LlamaService {\r\n  private static instance: LlamaService | null = null;\r\n  private openai: OpenAI;\r\n  private modelName: string;\r\n  private embeddingModelName: string = 'nomic-embed-text';\r\n  private messageQueue: QueuedMessage[] = [];\r\n  private isProcessing: boolean = false;\r\n\r\n  private constructor() {\r\n    debug('Constructing LlamaService');\r\n    dotenv.config();\r\n    this.modelName = process.env.XAI_MODEL || 'llama3.2';\r\n    this.openai = new OpenAI({\r\n      baseURL: 'http://localhost:11434/v1',\r\n      apiKey: 'ollama',\r\n      dangerouslyAllowBrowser: true\r\n    });\r\n    debug(`Using model: ${this.modelName}`);\r\n    debug('OpenAI client initialized');\r\n  }\r\n\r\n  public static getInstance(): LlamaService {\r\n    debug('Getting LlamaService instance');\r\n    if (!LlamaService.instance) {\r\n      debug('Creating new instance');\r\n      LlamaService.instance = new LlamaService();\r\n    }\r\n    return LlamaService.instance;\r\n  }\r\n\r\n  // Adding initializeModel method to satisfy ILlamaService interface\r\n  public async initializeModel(): Promise<void> {\r\n    debug('Initializing model...');\r\n    try {\r\n      // Placeholder for model setup if needed\r\n      debug(`Model ${this.modelName} initialized successfully.`);\r\n    } catch (error) {\r\n      debug('Error during model initialization:', error);\r\n      throw error;\r\n    }\r\n  }\r\n\r\n  async queueMessageCompletion(\r\n    context: string,\r\n    temperature: number,\r\n    stop: string[],\r\n    frequency_penalty: number,\r\n    presence_penalty: number,\r\n    max_tokens: number\r\n  ): Promise<any> {\r\n    debug('Queueing message completion');\r\n    return new Promise((resolve, reject) => {\r\n      this.messageQueue.push({\r\n        context,\r\n        temperature,\r\n        stop,\r\n        frequency_penalty,\r\n        presence_penalty,\r\n        max_tokens,\r\n        useGrammar: true,\r\n        resolve,\r\n        reject,\r\n      });\r\n      this.processQueue();\r\n    });\r\n  }\r\n\r\n  async queueTextCompletion(\r\n    context: string,\r\n    temperature: number,\r\n    stop: string[],\r\n    frequency_penalty: number,\r\n    presence_penalty: number,\r\n    max_tokens: number\r\n  ): Promise<string> {\r\n    debug('Queueing text completion');\r\n    return new Promise((resolve, reject) => {\r\n      this.messageQueue.push({\r\n        context,\r\n        temperature,\r\n        stop,\r\n        frequency_penalty,\r\n        presence_penalty,\r\n        max_tokens,\r\n        useGrammar: false,\r\n        resolve,\r\n        reject,\r\n      });\r\n      this.processQueue();\r\n    });\r\n  }\r\n\r\n  private async processQueue() {\r\n    debug(`Processing queue: ${this.messageQueue.length} items`);\r\n    if (this.isProcessing || this.messageQueue.length === 0) {\r\n      return;\r\n    }\r\n\r\n    this.isProcessing = true;\r\n\r\n    while (this.messageQueue.length > 0) {\r\n      const message = this.messageQueue.shift();\r\n      if (message) {\r\n        try {\r\n          const response = await this.getCompletionResponse(\r\n            message.context,\r\n            message.temperature,\r\n            message.stop,\r\n            message.frequency_penalty,\r\n            message.presence_penalty,\r\n            message.max_tokens,\r\n            message.useGrammar\r\n          );\r\n          message.resolve(response);\r\n        } catch (error) {\r\n          debug('Queue processing error:', error);\r\n          message.reject(error);\r\n        }\r\n      }\r\n    }\r\n\r\n    this.isProcessing = false;\r\n  }\r\n\r\n  private async getCompletionResponse(\r\n    context: string,\r\n    temperature: number,\r\n    stop: string[],\r\n    frequency_penalty: number,\r\n    presence_penalty: number,\r\n    max_tokens: number,\r\n    useGrammar: boolean\r\n  ): Promise<any | string> {\r\n    debug('Getting completion response');\r\n    try {\r\n      const completion = await this.openai.chat.completions.create({\r\n        model: this.modelName,\r\n        messages: [{ role: 'user', content: context }],\r\n        temperature,\r\n        max_tokens,\r\n        stop,\r\n        frequency_penalty,\r\n        presence_penalty,\r\n      });\r\n\r\n      const response = completion.choices[0].message.content;\r\n\r\n      if (useGrammar && response) {\r\n        try {\r\n          let jsonResponse = JSON.parse(response);\r\n          return jsonResponse;\r\n        } catch {\r\n          const jsonMatch = response.match(/```json\\s*([\\s\\S]*?)\\s*```/);\r\n          if (jsonMatch) {\r\n            try {\r\n              return JSON.parse(jsonMatch[1]);\r\n            } catch {\r\n              throw new Error(\"Failed to parse JSON from response\");\r\n            }\r\n          }\r\n          throw new Error(\"No valid JSON found in response\");\r\n        }\r\n      }\r\n\r\n      return response || '';\r\n    } catch (error) {\r\n      debug('Completion error:', error);\r\n      throw error;\r\n    }\r\n  }\r\n\r\n  async getEmbeddingResponse(input: string): Promise<number[] | undefined> {\r\n    debug('Getting embedding response');\r\n    try {\r\n      const embeddingResponse = await this.openai.embeddings.create({\r\n        model: this.embeddingModelName,\r\n        input,\r\n      });\r\n\r\n      return embeddingResponse.data[0].embedding;\r\n    } catch (error) {\r\n      debug('Embedding error:', error);\r\n      return undefined;\r\n    }\r\n  }\r\n}\r\n\r\ndebug('LlamaService module loaded');\r\nexport default LlamaService;\r\n`","created_at":"2024-11-03T04:52:25Z","url":"https://github.com/ai16z/eliza/issues/69#issuecomment-2453296314","type":"issue","issue_number":"69"},{"id":2453296570,"body":"need to add support through the new updated model providers rather than just replace llama-cpp","created_at":"2024-11-03T04:53:44Z","url":"https://github.com/ai16z/eliza/issues/69#issuecomment-2453296570","type":"issue","issue_number":"69"},{"id":2453444533,"body":"issues with image gen handling this way effect handling of text attachments. solution will be to have image gen save and return file path not a base64 string. then to have action handled ACTION = \"IMAGE GEN\"","created_at":"2024-11-03T14:18:49Z","url":"https://github.com/ai16z/eliza/issues/158#issuecomment-2453444533","type":"issue","issue_number":"158"},{"id":2460917086,"body":"I added a new OLLAMA model provider. Also there is a switch now for llama.ts if you are using local provider ollama or defaults to llama-cpp. u can set the Ollama mode provider to use a remote url if hosting remotely. Select the models and embedding models. Env variables to set are included in the .env.example  (also the image posting handling is in this code just dont merge that part in discord messages.ts)   https://github.com/o-on-x/eliza_ollama\r\n","created_at":"2024-11-06T22:26:40Z","url":"https://github.com/ai16z/eliza/issues/69#issuecomment-2460917086","type":"issue","issue_number":"69"},{"id":2462117021,"body":"setting this in packages/plugin-image-generation/tsconfig.json\r\n``` {\r\n    \"extends\": \"../../tsconfig.json\",\r\n    \"compilerOptions\": {\r\n      \"outDir\": \"dist\",\r\n      \"rootDir\": \".\",\r\n      \"module\": \"ESNext\",\r\n      \"moduleResolution\": \"Bundler\",\r\n      \"types\": [\"node\"]\r\n    },\r\n    \"include\": [\"src\"],\r\n    \"references\": [\r\n      { \"path\": \"../core\" }\r\n    ]\r\n  }\r\n```","created_at":"2024-11-07T12:29:51Z","url":"https://github.com/ai16z/eliza/issues/227#issuecomment-2462117021","type":"issue","issue_number":"227"},{"id":2463692870,"body":"latest push fixed this","created_at":"2024-11-08T03:37:49Z","url":"https://github.com/ai16z/eliza/issues/229#issuecomment-2463692870","type":"issue","issue_number":"229"},{"id":2463692978,"body":"closed","created_at":"2024-11-08T03:37:59Z","url":"https://github.com/ai16z/eliza/issues/229#issuecomment-2463692978","type":"issue","issue_number":"229"},{"id":2463693970,"body":"still issues with finding @eliza/core","created_at":"2024-11-08T03:39:20Z","url":"https://github.com/ai16z/eliza/issues/227#issuecomment-2463693970","type":"issue","issue_number":"227"},{"id":2463695147,"body":"twitter cache pointing to current directory only rather than searching up till find","created_at":"2024-11-08T03:40:57Z","url":"https://github.com/ai16z/eliza/issues/230#issuecomment-2463695147","type":"issue","issue_number":"230"},{"id":2466462082,"body":"this was the solved with adding the BASE_MINT=So11111111111111111111111111111111111111112 to .env","created_at":"2024-11-09T21:15:36Z","url":"https://github.com/ai16z/eliza/issues/238#issuecomment-2466462082","type":"issue","issue_number":"238"},{"id":2486560207,"body":"@sirkitree @monilpat i had added this as way to use ollama's embedding endpoints. issue is this switches the model loaded in ollama each time an embedding is needed. at the time of, there was no good local embedding solution. now there is fastembedding. so it makes sense to just have either fastembedding or openai. also this resolves a breaking issue with anyone using ollama on newest codebase. i had added this code, i was removing it now that a better solution exists  https://github.com/ai16z/eliza/pull/221/files ","created_at":"2024-11-19T19:20:06Z","url":"https://github.com/ai16z/eliza/pull/413#issuecomment-2486560207","type":"pr","issue_number":"413"}],"total_comments":14}},"summary":"o-on-x is a developer actively contributing to a GitHub repository with a focus on AI and tweet-related functionalities. Their recent work includes adding style guidelines to the context, removing specific embeddings, setting post times, and addressing issues related to bot interactions and response handling. They seem to be engaged in backend development, bug fixes, and feature enhancements, with a particular emphasis on improving tweet generation and AI functionality.","score":163,"score_breakdown":{"merged_prs":140,"issues":6,"pr_commits":10,"pr_reviews":0,"comments":7,"total":163}};
    </script>
    <script type="text/javascript">
        
const StatusDot = ({ status }) => {
  const colors = {
    open: 'bg-green-500',
    closed: 'bg-red-500',
    merged: 'bg-purple-500'
  };

  return React.createElement('span', {
    className: `inline-block w-2 h-2 rounded-full ${colors[status]} mr-2`
  });
};

const ActivitySection = ({ title, items = [], showStatus = false }) => {
  const [isExpanded, setIsExpanded] = React.useState(false);
  
  const getStatus = (item) => {
    if (item.state === 'merged' || (item.state === 'closed' && title === 'Pull Requests')) {
      return 'merged';
    }
    return item.state || 'open';
  };

  return React.createElement('div', { className: 'border rounded-lg p-4' },
    React.createElement('div', {
      className: 'flex items-center justify-between cursor-pointer',
      onClick: () => setIsExpanded(!isExpanded)
    },
      React.createElement('h3', { className: 'font-semibold' }, title),
      React.createElement('span', null, isExpanded ? '▼' : '▶')
    ),
    isExpanded && React.createElement('div', { className: 'mt-4 space-y-2' },
      items.map((item, index) => 
        React.createElement('div', { key: index, className: 'p-2 hover:bg-gray-50 dark:hover:bg-gray-800 rounded' },
          React.createElement('a', {
            href: item.url,
            target: '_blank',
            rel: 'noopener noreferrer',
            className: 'text-sm hover:text-blue-500 flex flex-col gap-1'
          },
            React.createElement('span', { className: 'font-medium flex items-center' },
              showStatus && React.createElement(StatusDot, { status: getStatus(item) }),
              item.message || item.title || item.body
            ),
            React.createElement('span', { className: 'text-gray-500 text-xs' },
              new Date(item.date || item.created_at).toLocaleDateString()
            )
          )
        )
      )
    )
  );
};

const StatCard = ({ name, value }) => {
  return React.createElement('div', { className: 'bg-white dark:bg-gray-800 rounded-lg p-6 shadow' },
    React.createElement('h3', { className: 'font-semibold' }, name),
    React.createElement('p', { className: 'text-2xl font-bold' }, value)
  );
};

const ContributorProfile = ({ data }) => {
  const stats = [
    { name: 'Commits', value: data.activity.code.total_commits },
    { name: 'Pull Requests', value: data.activity.code.total_prs },
    { name: 'Issues', value: data.activity.issues.total_opened },
    { name: 'Comments', value: data.activity.engagement.total_comments }
  ];

  return React.createElement('div', { className: 'max-w-7xl mx-auto p-4 space-y-6' },
    React.createElement('div', { className: 'bg-white dark:bg-gray-800 rounded-lg p-6 shadow' },
      React.createElement('div', { className: 'flex items-center justify-between' },
        React.createElement('div', { className: 'flex items-center gap-4' },
          React.createElement('img', {
            src: data.avatar_url,
            alt: `${data.contributor}'s avatar`,
            className: 'w-16 h-16 rounded-full'
          }),
          React.createElement('div', null,
            React.createElement('h1', { className: 'text-2xl font-bold' }, data.contributor),
            React.createElement('div', { className: 'text-gray-600 dark:text-gray-400' },
              React.createElement('span', { className: 'font-semibold' }, 'Score: '),
              data.score
            )
          )
        )
      )
    ),

    data.summary && React.createElement('div', { 
      className: 'bg-white dark:bg-gray-800 rounded-lg p-6 shadow'
    },
      React.createElement('p', { 
        className: 'text-gray-700 dark:text-gray-300 text-sm leading-relaxed'
      }, data.summary)
    ),

    React.createElement('div', { className: 'grid grid-cols-1 md:grid-cols-4 gap-4' },
      stats.map(stat => React.createElement(StatCard, { 
        key: stat.name,
        ...stat
      }))
    ),

    React.createElement('div', { className: 'space-y-4' },
      React.createElement(ActivitySection, {
        title: 'Commits',
        items: data.activity.code.commits
      }),
      React.createElement(ActivitySection, {
        title: 'Pull Requests',
        items: data.activity.code.pull_requests,
        showStatus: true
      }),
      React.createElement(ActivitySection, {
        title: 'Issues',
        items: data.activity.issues.opened || [],
        showStatus: true
      }),
      React.createElement(ActivitySection, {
        title: 'Comments',
        items: data.activity.engagement.comments
      })
    )
  );
};

// Initialize React root and render
const root = ReactDOM.createRoot(document.getElementById('root'));
root.render(React.createElement(ContributorProfile, { data: window.__DATA__ }));
    </script>
</body>
</html>